{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS='/Users/adammckenna/protein_structure_prediction_DeepLearning/psp-keras-training.json'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import auth as google_auth\n",
    "  google_auth.authenticate_user()\n",
    "\n",
    "# If you are running this notebook locally, replace the string below with the\n",
    "# path to your service account key and run this cell to authenticate your GCP\n",
    "# account.\n",
    "else:\n",
    "  %env GOOGLE_APPLICATION_CREDENTIALS '/Users/adammckenna/protein_structure_prediction_DeepLearning/psp-keras-training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'keras-python-models' #@param {type:\"string\"}\n",
    "REGION = 'us-central1' #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1766452880  2020-07-18T13:49:00Z  gs://keras-python-models/cullpdb+profile_6133_filtered.npy#1595080140705967  metageneration=2\r\n",
      "                                 gs://keras-python-models/JOB36/\r\n",
      "                                 gs://keras-python-models/JOB37/\r\n",
      "                                 gs://keras-python-models/JOB38/\r\n",
      "                                 gs://keras-python-models/JOB39/\r\n",
      "                                 gs://keras-python-models/JOB40/\r\n",
      "                                 gs://keras-python-models/JOB41/\r\n",
      "                                 gs://keras-python-models/JOB42/\r\n",
      "                                 gs://keras-python-models/JOB43/\r\n",
      "                                 gs://keras-python-models/JOB44/\r\n",
      "                                 gs://keras-python-models/JOB45/\r\n",
      "                                 gs://keras-python-models/JOB46/\r\n",
      "                                 gs://keras-python-models/JOB47/\r\n",
      "                                 gs://keras-python-models/JOB48/\r\n",
      "                                 gs://keras-python-models/JOB49/\r\n",
      "                                 gs://keras-python-models/JOB50/\r\n",
      "                                 gs://keras-python-models/JOB51/\r\n",
      "                                 gs://keras-python-models/JOB52/\r\n",
      "                                 gs://keras-python-models/JOB53/\r\n",
      "                                 gs://keras-python-models/JOB54/\r\n",
      "                                 gs://keras-python-models/JOB55/\r\n",
      "                                 gs://keras-python-models/JOB56/\r\n",
      "                                 gs://keras-python-models/JOB57/\r\n",
      "                                 gs://keras-python-models/JOB58/\r\n",
      "                                 gs://keras-python-models/JOB59/\r\n",
      "                                 gs://keras-python-models/JOB60/\r\n",
      "                                 gs://keras-python-models/logs/\r\n",
      "TOTAL: 1 objects, 1766452880 bytes (1.65 GiB)\r\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir/venv/bin:/Users/adammckenna/Downloads/google-cloud-sdk/bin:/Users/adammckenna/anaconda3/bin:/Users/adammckenna/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/go/bin:/Applications/apache-maven-3.6.2/bin\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! This command will take you through the configuration of gcloud.\n",
      "\n",
      "Settings from your current configuration [default] are:\n",
      "compute:\n",
      "  region: europe-west1\n",
      "  zone: europe-west1-b\n",
      "core:\n",
      "  account: mckenna45678@hotmail.co.uk\n",
      "  disable_usage_reporting: 'True'\n",
      "  project: psp-keras-training\n",
      "ml_engine:\n",
      "  local_python: /Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir/venv/bin/python3\n",
      "\n",
      "Pick configuration to use:\n",
      " [1] Re-initialize this configuration [default] with new settings \n",
      " [2] Create a new configuration\n",
      "Please enter your numeric choice:  ^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir\n",
      "/Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir/training/psp_keras_training.json\n",
      "Doing something\n",
      "2020-08-06 15:17:47.357339: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-08-06 15:17:47.388388: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbbf0ed55c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-06 15:17:47.388420: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "cwd from top of psp_lstm_gcp file  /Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir\n",
      "Something\n",
      "10\n",
      "cwd from psp_lstm_gcp dir  /Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir\n",
      "job_dir gs://keras-python-models\n",
      "/Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir\n",
      "/Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir cwd before training data\n",
      "Loading test dataset (CB513)...\n",
      "\n",
      "(514, 700, 21)\n",
      "(514, 700, 21)\n",
      "(514, 700, 8)\n",
      "New testhot shape (257, 700, 21)\n",
      "New testpssm shape (257, 700, 21)\n",
      "New testlabel shape (257, 700, 8)\n",
      "10\n",
      "Fitting model...\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "#!export GOOGLE_APPLICATION_CREDENTIALS='/Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir/service-account.json'\n",
    "!gcloud ai-platform local train \\\n",
    "  --module-name training.task \\\n",
    "  --package-path training/ \n",
    "# !gcloud auth application-default login\n",
    "\n",
    "#  --filename psp_keras_training.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date\n",
    "y = ('model_1_hpconfig' + str(datetime.date(datetime.now())) + '.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_1_hpconfig2020-08-06.h5'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('psp_keras_training.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"type\": \"service_account\",\\n  \"project_id\": \"psp-keras-training\",\\n  \"private_key_id\": \"f67cd58902550fe0acaf693bd8e4b1cb31a20f66\",\\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCpxk6xw8k35Zyb\\\\nXTOn3yNpOAB2/5F/NE2ksruc2h8aeAbqa5gKLJatshlB5zM0iezpFnJfv4Y4GL62\\\\nzroHBaVj0UwXdUSG2oR5h3sR/O1HK+z92JegfKb15XTXzBkd9HDTv8JGby5nfoeq\\\\ny1HZL1BrKs8N6ZahWptemdJ7RL9wUdSoE2lo3ByuEBvNvEsMTf9lHDm+hqR9zfRS\\\\nmla4HSbj5tQHab0Zay0nK6oAmg6JdWFN5mExTuHcIDUgl4cPucsug9XGWfvTU5FC\\\\n9X1Z44MYNCNBkubtNjC8abRIoBWE1sapY1+ud0CSwM1cYYA4E2KakCIsUgQA6hIX\\\\nMb+w4hPxAgMBAAECggEAOFZVm9i354QaxVWWxCCgPBS41sQ1EGyXTU2AQ2sAQkTN\\\\nKrIciFfGgookpoFltQwF4U0xFAPWsrvHYZtVEOp3ORnlca1Y2ZUeu8NQ/CWdyW3S\\\\nnWPn4PosX1HOxaN1PEL2w9WQztWrJO3QcEU6+mKggYd5oQjAEvghsbFCruQf0NHT\\\\nnIP25JxIheG3RV8XHTjLjnBYVOW/xrpcyuY20EDQWtsOVWMiaTvkJElWdifPCOqf\\\\nsxONb2o/FS/ZSqNi02DoYFOUJ7hS76ODABltuGBiYD4mNIAZI5DqP7w7j8zHtBEF\\\\nnb5T+KL8HGBPhm1QRglwaf1flgF1H1EC9SIaxpmlfwKBgQDocmHS3zDpS/xPn323\\\\nfvrMrcP35jCODFwWrlJ06+mCZdO6leOrzP4m75la9CtSBxb/BkE4tKzFf/Mrz6UW\\\\nCI/NuOZ3TBR5dS0yzvhRgkBApOcNJGnZKaLwpgL3Xut/sj534xOh4E+0c1GQ3UbS\\\\nJc18VvvtSBVooEcna43U/IIexwKBgQC6+jktw9LYb0/A9h6lfPOy5Jhum+4XMN7q\\\\nmtuO99ZJo2cpwOSo2hMD5WCi2wlLIrY9J/uaDvBNgzqD1XNSZUAonGJz/Ijqrlr6\\\\n2ndffG5hEQVVgeQhU4Kbj9MApUMXMWBMAtjPz2ccQRiQbJz21qXm+hwc+q82YwfC\\\\ndpvuUmhfhwKBgBZaGWoLqEpm0iZd8drZTQitlPCbpAOk0J4luSwhG108qXWBwFje\\\\n53mbBKYoYJoMsHtpgJARX6PGP/3wg7FwtVNU+mRyXm2nhOcj2+EKo18T/FvZKoyF\\\\nYnO9ov9ik0a+Px+MKHEifsedZzSE8rB2jiHsw2D304cyEmWKBCN8/I41AoGAfWsK\\\\nn6AS9gr0PELbG7ZM4VpkxQzjD7tAS63ESIoFuy9YK7Gvr3NX/e0GfTbfbP7DrxH6\\\\nusP6fIPh5DxxNnukJZmAkLx8Oq7paArSt8IHaCS6pziRhak2mpJGvtAbyJreNR5T\\\\nwFEmoNUpXPb98rTYY66w3N9qV9jlg5N71MiCKlECgYA99xKY2NRnCwlHmQDMG7Du\\\\ni8HrZLDMxZW5PrCUNOAaZwqsr1RgAjgQ0UKCcoin8Ur6Nm+7/fCaUccErtXtqpTr\\\\nKADPpKLZXKRx7PKyuJmP0kTuKhmYNXtPD/1g4X4WiEKYb0bNnZIcm9EYz4e/5k/H\\\\nsg0S15YHr/TPEwdH9dr5oA==\\\\n-----END PRIVATE KEY-----\\\\n\",\\n  \"client_email\": \"ai-platform-service-account@psp-keras-training.iam.gserviceaccount.com\",\\n  \"client_id\": \"107219595993102747189\",\\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/ai-platform-service-account%40psp-keras-training.iam.gserviceaccount.com\"\\n}\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB72] submitted successfully.\r\n",
      "Your job is still active. You may view the status of your job with the command\r\n",
      "\r\n",
      "  $ gcloud ai-platform jobs describe JOB72\r\n",
      "\r\n",
      "or continue streaming the logs with the command\r\n",
      "\r\n",
      "  $ gcloud ai-platform jobs stream-logs JOB72\r\n",
      "jobId: JOB72\r\n",
      "state: QUEUED\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform jobs submit training JOB72 \\\n",
    "    --package-path training/ \\\n",
    "    --module-name training.task \\\n",
    "    --staging-bucket gs://keras-python-models \\\n",
    "    --region us-central1 \\\n",
    "    --config training/temp_gcp_configfile.yaml \\\n",
    "    --runtime-version 2.1 \\\n",
    "    --python-version 3.7 \\\n",
    "    --job-dir gs://keras-python-models \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS='psp-keras-training.json'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "    google_auth.authenticate_user()\n",
    "\n",
    "# If you are running this notebook locally, replace the string below with the\n",
    "# path to your service account key and run this cell to authenticate your GCP\n",
    "# account.\n",
    "else:\n",
    "    %env GOOGLE_APPLICATION_CREDENTIALS 'psp-keras-training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy>=1.18.0\n",
      "  Using cached numpy-1.19.1-cp37-cp37m-macosx_10_9_x86_64.whl (15.3 MB)\n",
      "Collecting pandas>=0.25.3\n",
      "  Downloading pandas-1.1.0-cp37-cp37m-macosx_10_9_x86_64.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in ./venv/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.15.0)\n",
      "Collecting tensorflow==2.1.0\n",
      "  Downloading tensorflow-2.1.0-cp37-cp37m-macosx_10_11_x86_64.whl (120.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 120.8 MB 11.2 MB/s eta 0:00:01    |███████████████████▊            | 74.3 MB 8.3 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting h5py==2.10.0\n",
      "  Using cached h5py-2.10.0-cp37-cp37m-macosx_10_6_intel.whl (3.0 MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./venv/lib/python3.7/site-packages (from pandas>=0.25.3->-r requirements.txt (line 2)) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./venv/lib/python3.7/site-packages (from pandas>=0.25.3->-r requirements.txt (line 2)) (2.8.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Processing /Users/adammckenna/Library/Caches/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e/absl_py-0.9.0-py3-none-any.whl\n",
      "Processing /Users/adammckenna/Library/Caches/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6/wrapt-1.12.1-cp37-cp37m-macosx_10_7_x86_64.whl\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in ./venv/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements.txt (line 4)) (3.12.4)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Using cached scipy-1.4.1-cp37-cp37m-macosx_10_6_intel.whl (28.4 MB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting wheel>=0.26; python_version >= \"3\"\n",
      "  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Processing /Users/adammckenna/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2/termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.30.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.8 MB)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in ./venv/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (40.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./venv/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (1.20.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (2.24.0)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./venv/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in ./venv/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (4.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (2020.6.20)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in ./venv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 4)) (3.1.0)\n",
      "Using legacy 'setup.py install' for gast, since package 'wheel' is not installed.\n",
      "Installing collected packages: numpy, pandas, opt-einsum, absl-py, wrapt, h5py, keras-applications, tensorflow-estimator, scipy, keras-preprocessing, wheel, gast, google-pasta, termcolor, astor, grpcio, werkzeug, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, tensorflow\n",
      "    Running setup.py install for gast ... \u001b[?25ldone\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorboard 2.1.1 requires setuptools>=41.0.0, but you'll have setuptools 40.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hSuccessfully installed absl-py-0.9.0 astor-0.8.1 gast-0.2.2 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.19.1 oauthlib-3.1.0 opt-einsum-3.3.0 pandas-1.1.0 requests-oauthlib-1.3.0 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "checkpoint_path = \"gs://keras-python-models/checkpoints/\" + str(datetime.date(datetime.now())) + \".h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://keras-python-models/checkpoints/2020-08-03.h5'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'getset_descriptor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-1987c9316de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'getset_descriptor' object is not callable"
     ]
    }
   ],
   "source": [
    "datetime.minute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2020-08-13 14:55:32.912978\n",
      "date and time = 13/08/2020 14:55:32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "import argparse\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Embedding, LSTM, Dense, Dropout, Activation, Convolution2D, GRU, Concatenate, Reshape,MaxPooling1D, Conv2D, MaxPooling2D,Convolution1D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping ,ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_pssm_model():\n",
    "\n",
    "    #main input is the length of the amino acid in the protein sequence (700,)\n",
    "    main_input = Input(shape=(700,21), dtype='float32', name='main_input')\n",
    "\n",
    "    #get shape of input layers\n",
    "    print (main_input.get_shape())\n",
    "\n",
    "    #concatenate input layers\n",
    "    concat = main_input\n",
    "\n",
    "    # #1D Convolutional layer and maxpooling to downsample features\n",
    "    # conv1_features = Conv1D(42,1,strides=1,activation='relu', padding='same')(concat)\n",
    "    # max_pool_1d = MaxPooling1D(pool_size = 2, strides =1, padding='same')(conv1_features)\n",
    "\n",
    "    # print ('conv1_features shape', conv1_features.get_shape())\n",
    "    #print ('max_pool_1d shape', max_pool_1d.get_shape())\n",
    "\n",
    "    #reshape 1D convolutional layer\n",
    "    conv1_features = Reshape((700, 42, 1))(concat)\n",
    "\n",
    "    #2D Convolutional layers and 2D Max Pooling\n",
    "    conv2_features = Conv2D(42,3,strides=1,activation='relu', padding='same')(conv1_features)\n",
    "    print ('Conv2D layer1 shape',conv2_features.get_shape())\n",
    "\n",
    "    max_pool_2D = MaxPooling2D(pool_size=(2,2), strides=1, padding ='same')(conv2_features)\n",
    "    max_pool_2D = Dropout(0.5)(max_pool_2D)\n",
    "    print ('MaxPooling Shape', max_pool_2D.get_shape())\n",
    "\n",
    "    #2D Convolutional layers and 2D Max Pooling\n",
    "    conv2_features = Conv2D(84,3,strides=1,activation='relu', padding='same')(max_pool_2D)\n",
    "    max_pool_2D = MaxPooling2D(pool_size=(2,2), strides=1, padding ='same')(conv2_features)\n",
    "    # conv2_features = Conv2D(42,3,strides=1,activation='relu', padding='same')(max_pool_2D)\n",
    "\n",
    "    max_pool_2D = Dropout(0.5)(max_pool_2D)\n",
    "    print ('Conv2D layer1 shape',conv2_features.get_shape())\n",
    "\n",
    "    #conv2_batch_normal = BatchNormalization()(conv2_features)\n",
    "    #max_pool_2D = MaxPooling2D(pool_size=(1, 2), strides=None, border_mode='same')(conv2_batch_normal)\n",
    "\n",
    "    #conv2_features = Convolution2D(84,3,1,activation='relu', padding='same')(conv2_features)\n",
    "    # print 'conv2_features.get_shape()', conv2_features.get_shape()\n",
    "\n",
    "    #reshape 2D convolutional layers\n",
    "    conv2_features = Reshape((700, 84*42))(max_pool_2D)\n",
    "\n",
    "    #conv2_features = Dense(500, activation='relu')(conv2_features)\n",
    "\n",
    "    #Long Short Term Memory layers with tanh activation, sigmoid recurrent activiation and dropout of 0.5\n",
    "    lstm_f1 = LSTM(400,return_sequences=True,activation = 'tanh', recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(conv2_features)\n",
    "    lstm_b1 = LSTM(400, return_sequences=True, activation='tanh',go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(conv2_features)\n",
    "\n",
    "    lstm_f2 = LSTM(300, return_sequences=True,activation = 'tanh',recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(lstm_f1)\n",
    "    lstm_b2 = LSTM(300, return_sequences=True,activation='tanh', go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(lstm_b1)\n",
    "\n",
    "    #concatenate LSTM with convolutional layers\n",
    "    concat_features = Concatenate(axis=-1)([lstm_f2, lstm_b2, conv2_features])\n",
    "    concat_features = Dropout(0.4)(concat_features)\n",
    "\n",
    "    #Dense layers\n",
    "    #protein_features = Dense(600,activation='relu')(concat_features)\n",
    "    #protein_features = Dropout(0.4)(protein_features)\n",
    "\n",
    "    # protein_features = TimeDistributedDense(600,activation='relu')(concat_features)\n",
    "    # protein_features = TimeDistributedDense(100,activation='relu', W_regularizer=l2(0.001))(protein_features)\n",
    "    #protein_features_2 = Dense(300,activation='relu')(protein_features)\n",
    "    #protein_features_2 = Dropout(0.4)(protein_features_2)\n",
    "\n",
    "    #Final Dense layer with 8 nodes for the 8 output classifications\n",
    "    main_output = Dense(8, activation='softmax', name='main_output')(concat_features)\n",
    "\n",
    "    #create model from inputs and outputs\n",
    "    model = Model(inputs=[main_input], outputs=[main_output])\n",
    "    #use Adam optimizer\n",
    "    adam = Adam(lr=0.003)\n",
    "    #Adam is fast, but tends to over-fit\n",
    "    #SGD is low but gives great results, sometimes RMSProp works best, SWA can easily improve quality, AdaTune\n",
    "\n",
    "    #compile model using adam optimizer and the cateogorical crossentropy loss function\n",
    "    model.compile(optimizer = adam, loss={'main_output': 'categorical_crossentropy'}, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "    # load_file = \"./model/ac_LSTM_best_time_17.h5\" # M: weighted_accuracy E: val_weighted_accuracy\n",
    "    checkpoint_path = \"gs://keras-python-models/checkpoints/\" + str(datetime.date(datetime.now())) + \".h5\"\n",
    "    checkpointer = ModelCheckpoint(filepath=checkpoint_path,verbose=1,save_best_only=True, monitor='val_acc', mode='max')\n",
    "#     checkpointer = ModelCheckpoint(filepath=checkpoint_path,verbose=1,save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 700, 21)\n",
      "WARNING:tensorflow:From /Users/adammckenna/protein_structure_prediction_DeepLearning/psp_gcp_test_dir/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Conv2D layer1 shape (?, 700, 42, 42)\n",
      "MaxPooling Shape (?, 700, 42, 42)\n",
      "Conv2D layer1 shape (?, 700, 42, 84)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         [(None, 700, 21)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 700, 42, 1)   0           main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 700, 42, 42)  420         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 700, 42, 42)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 700, 42, 42)  0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 700, 42, 84)  31836       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 700, 42, 84)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 700, 42, 84)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 700, 3528)    0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 700, 400)     6286400     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 700, 400)     6286400     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 700, 300)     841200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 700, 300)     841200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 700, 4128)    0           lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 700, 4128)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 700, 8)       33032       dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,320,488\n",
      "Trainable params: 14,320,488\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_lstm_pssm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
